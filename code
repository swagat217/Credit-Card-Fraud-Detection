import numpy as nphttps://github.com/swagat217/Credit-Card-Fraud-Detection/tree/main
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import SGD, Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping
from imblearn.over_sampling import SMOTE
import time

# Set random seed for reproducibility
np.random.seed(42)

# 1. Load the dataset
print("Loading dataset...")
df = pd.read_csv('creditcard.csv')

# 2. Explore the dataset
print("Dataset shape:", df.shape)
print("\nClass distribution:")
print(df['Class'].value_counts())

# Check for missing values
print("\nMissing values:")
print(df.isnull().sum().sum())

# 3. Data Preprocessing
# Separating features and target
X = df.drop('Class', axis=1)
y = df['Class']

# Normalize/Standardize features
print("\nNormalizing features...")
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Handle class imbalance using SMOTE (only on training data)
print("\nHandling class imbalance with SMOTE...")
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print("Original training set shape:", X_train.shape)
print("Balanced training set shape:", X_train_balanced.shape)
print("Class distribution after balancing:")
print(pd.Series(y_train_balanced).value_counts())

# 4. Create a function to build and train the model with different configurations
def build_and_train_model(hidden_activation, optimizer_name, X_train, y_train, X_test, y_test):
    # Define model architecture
    model = Sequential()
    model.add(Dense(64, activation=hidden_activation, input_shape=(X_train.shape[1],)))
    model.add(Dropout(0.3))
    model.add(Dense(32, activation=hidden_activation))
    model.add(Dropout(0.3))
    model.add(Dense(16, activation=hidden_activation))
    model.add(Dense(1, activation='sigmoid'))
    
    # Setup optimizer
    if optimizer_name == 'sgd':
        optimizer = SGD(learning_rate=0.01)
    elif optimizer_name == 'adam':
        optimizer = Adam(learning_rate=0.001)
    elif optimizer_name == 'rmsprop':
        optimizer = RMSprop(learning_rate=0.001)
        
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    
    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    
    # Start time for measuring training time
    start_time = time.time()
    
    # Train the model
    history = model.fit(
        X_train, y_train,
        epochs=30,
        batch_size=256,
        validation_split=0.2,
        callbacks=[early_stopping],
        verbose=0
    )
    
    # Calculate training time
    training_time = time.time() - start_time
    
    # Start time for measuring inference time
    start_time = time.time()
    
    # Make predictions
    y_pred_proba = model.predict(X_test, verbose=0)
    y_pred = (y_pred_proba > 0.5).astype(int)
    
    # Calculate inference time
    inference_time = time.time() - start_time
    
    # Calculate metrics
    report = classification_report(y_test, y_pred, output_dict=True)
    
    # Get accuracy and loss history
    acc_history = history.history['accuracy']
    val_acc_history = history.history['val_accuracy']
    loss_history = history.history['loss']
    val_loss_history = history.history['val_loss']
    
    # Calculate ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    
    # Calculate Precision-Recall curve
    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
    pr_auc = auc(recall, precision)
    
    return {
        'model': model,
        'history': history,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba,
        'report': report,
        'training_time': training_time,
        'inference_time': inference_time,
        'acc_history': acc_history,
        'val_acc_history': val_acc_history,
        'loss_history': loss_history,
        'val_loss_history': val_loss_history,
        'roc': (fpr, tpr, roc_auc),
        'pr': (precision, recall, pr_auc)
    }

# 5. Train models with different configurations
print("\nTraining models with different configurations...")

# Define configurations to test
activations = ['relu', 'tanh']
optimizers = ['sgd', 'adam', 'rmsprop']

results = {}

for activation in activations:
    for optimizer in optimizers:
        config_name = f"{activation}_{optimizer}"
        print(f"\nTraining model with activation: {activation}, optimizer: {optimizer}")
        
        results[config_name] = build_and_train_model(
            activation, optimizer, 
            X_train_balanced, y_train_balanced, 
            X_test, y_test
        )
        
        # Print classification report
        print(f"\nClassification Report for {config_name}:")
        print(classification_report(y_test, results[config_name]['y_pred']))
        
        # Print training and inference times
        print(f"Training time: {results[config_name]['training_time']:.2f} seconds")
        print(f"Inference time: {results[config_name]['inference_time']:.2f} seconds")
        print(f"F1 Score: {results[config_name]['report']['1']['f1-score']:.4f}")
        print(f"ROC AUC: {results[config_name]['roc'][2]:.4f}")
        print(f"PR AUC: {results[config_name]['pr'][2]:.4f}")

# 6. Visualize results

# 6.1. Plot accuracy and loss for each model
plt.figure(figsize=(20, 15))

plot_idx = 1
for activation in activations:
    for optimizer in optimizers:
        config_name = f"{activation}_{optimizer}"
        result = results[config_name]
        
        # Plot accuracy
        plt.subplot(len(activations), len(optimizers) * 2, plot_idx)
        plt.plot(result['acc_history'], label='Training Accuracy')
        plt.plot(result['val_acc_history'], label='Validation Accuracy')
        plt.title(f'Accuracy - {activation} + {optimizer}')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        
        # Plot loss
        plt.subplot(len(activations), len(optimizers) * 2, plot_idx + 1)
        plt.plot(result['loss_history'], label='Training Loss')
        plt.plot(result['val_loss_history'], label='Validation Loss')
        plt.title(f'Loss - {activation} + {optimizer}')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        
        plot_idx += 2

plt.tight_layout()
plt.savefig('accuracy_loss_plots.png')

# 6.2. Plot ROC curves for all models
plt.figure(figsize=(12, 10))
for activation in activations:
    for optimizer in optimizers:
        config_name = f"{activation}_{optimizer}"
        fpr, tpr, roc_auc = results[config_name]['roc']
        plt.plot(fpr, tpr, label=f'{config_name} (AUC = {roc_auc:.4f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
plt.savefig('roc_curves.png')

# 6.3. Plot Precision-Recall curves for all models
plt.figure(figsize=(12, 10))
for activation in activations:
    for optimizer in optimizers:
        config_name = f"{activation}_{optimizer}"
        precision, recall, pr_auc = results[config_name]['pr']
        plt.plot(recall, precision, label=f'{config_name} (AUC = {pr_auc:.4f})')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curves')
plt.legend(loc="lower left")
plt.savefig('pr_curves.png')

# 6.4. Create comparison bar charts for key metrics
metrics = ['f1-score', 'precision', 'recall', 'training_time', 'inference_time', 'roc_auc', 'pr_auc']
labels = []
f1_scores = []
precisions = []
recalls = []
training_times = []
inference_times = []
roc_aucs = []
pr_aucs = []

for activation in activations:
    for optimizer in optimizers:
        config_name = f"{activation}_{optimizer}"
        result = results[config_name]
        
        labels.append(config_name)
        f1_scores.append(result['report']['1']['f1-score'])
        precisions.append(result['report']['1']['precision'])
        recalls.append(result['report']['1']['recall'])
        training_times.append(result['training_time'])
        inference_times.append(result['inference_time'])
        roc_aucs.append(result['roc'][2])
        pr_aucs.append(result['pr'][2])

# Create comparison chart
plt.figure(figsize=(15, 12))

# F1 Scores
plt.subplot(3, 2, 1)
plt.bar(labels, f1_scores)
plt.title('F1 Scores')
plt.xticks(rotation=45)
plt.ylim(0, 1)

# Precision
plt.subplot(3, 2, 2)
plt.bar(labels, precisions)
plt.title('Precision')
plt.xticks(rotation=45)
plt.ylim(0, 1)

# Recall
plt.subplot(3, 2, 3)
plt.bar(labels, recalls)
plt.title('Recall')
plt.xticks(rotation=45)
plt.ylim(0, 1)

# ROC AUC
plt.subplot(3, 2, 4)
plt.bar(labels, roc_aucs)
plt.title('ROC AUC')
plt.xticks(rotation=45)
plt.ylim(0, 1)

# Training Time
plt.subplot(3, 2, 5)
plt.bar(labels, training_times)
plt.title('Training Time (s)')
plt.xticks(rotation=45)

# Inference Time
plt.subplot(3, 2, 6)
plt.bar(labels, inference_times)
plt.title('Inference Time (s)')
plt.xticks(rotation=45)

plt.tight_layout()
plt.savefig('metrics_comparison.png')

# 7. Generate confusion matrices for all models
plt.figure(figsize=(20, len(activations) * 5))
plot_idx = 1
for activation in activations:
    for optimizer in optimizers:
        config_name = f"{activation}_{optimizer}"
        result = results[config_name]
        
        cm = confusion_matrix(y_test, result['y_pred'])
        
        plt.subplot(len(activations), len(optimizers), plot_idx)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
        plt.title(f'Confusion Matrix\n{activation} + {optimizer}')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        
        plot_idx += 1

plt.tight_layout()
plt.savefig('confusion_matrices.png')

# Print overall best model
f1_scores = [(k, results[k]['report']['1']['f1-score']) for k in results.keys()]
best_model = max(f1_scores, key=lambda x: x[1])
print(f"\nBest model based on F1 score: {best_model[0]} with F1 score: {best_model[1]:.4f}")

# 8. Save the best model
best_model_name = best_model[0]
results[best_model_name]['model'].save(f'best_fraud_detection_model_{best_model_name}.h5')
print(f"Best model saved as best_fraud_detection_model_{best_model_name}.h5")

print("\nFraud Detection Analysis Complete!")
